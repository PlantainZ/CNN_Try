{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd,gluon,init,nd\n",
    "from mxnet.gluon import loss as gloss,nn,data as gdata\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BN层定义\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    if not autograd.is_training():\n",
    "        X_hat = (X--moving_mean) / nd.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(axis=0)\n",
    "            var = (X-mean**2).mean(axis=0)\n",
    "        else:\n",
    "            mean = X.mean(axis=(0,2,3),keepdims=True)\n",
    "            var = ((X-mean)**2).mean(axis=(0,2,3),keepdims=True)\n",
    "        X_hat = (X-mean)/nd.sqrt(var + eps)\n",
    "        \n",
    "        moving_mean = momentum * moving_mean + (1-momentum) * moving_mean\n",
    "        moving_var = momentum * moving_var + (1-momentum) + moving_var\n",
    "    \n",
    "    Y = gamma * X_hat + beta\n",
    "    \n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Block):\n",
    "    def __init__(self,num_features,num_dims,**kwargs):\n",
    "        super(BatchNorm,self).__init__(**kwargs)\n",
    "        \n",
    "        if num_dims ==2:\n",
    "            shape = (1,num_features)\n",
    "        else:\n",
    "            shape = (1,num_features,1,1)\n",
    "        self.gamma = self.params.get('gamma',shape = shape,init=init.One())\n",
    "        self.beta = self.params.get('beta',shape = shape,init=init.Zero())\n",
    "        \n",
    "        self.moving_mean = nd.zeros(shape)\n",
    "        self.moving_var = nd.zeros(shape)\n",
    "        \n",
    "        if num_dims == 2:\n",
    "            shape = (1,num_features)\n",
    "            \n",
    "            \n",
    "    def forward(self,X):\n",
    "        if self.moving_mean.context != X.context:\n",
    "            self.moving_mean = self.moving_mean.copyto(X.context)\n",
    "            self.moving_var = self.moving_var.copyto(X.context)\n",
    "        \n",
    "        Y,self.moving_mean,self.moving_var = batch_norm(X,self.gamma.data(),self.beta.data(),\n",
    "                                                       self.moving_mean,self.moving_var,\n",
    "                                                       eps=1e-5,momentum=0.9)\n",
    "        return Y \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 读取数据 + 生成iter \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),batch_size=batch_size,shuffle=True)\n",
    "test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 定义网络\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(6,kernel_size=5), # 注意，输入的不是一位线性数据，而是一张图像\n",
    "        BatchNorm(6,num_dims=4),# 所以这里的dims是4，（批次，channel,长宽）。注意前面的6和conv的输出通道数要对应。\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       \n",
    "       nn.Conv2D(16,kernel_size=5),\n",
    "       BatchNorm(16,num_dims=4),\n",
    "       nn.Activation('sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2,strides=2),\n",
    "       \n",
    "       nn.Dense(120),\n",
    "       BatchNorm(120,num_dims=2), # 注意这里的num_features也是120，但是在全连层，dims=2！！只有行和列。行是批次。\n",
    "       nn.Activation('sigmoid'),\n",
    "       \n",
    "       nn.Dense(84),\n",
    "       BatchNorm(84,num_dims=2),\n",
    "       nn.Activation('sigmoid'),\n",
    "       \n",
    "       nn.Dense(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv19 output shape:\t (1, 6, 24, 24)\n",
      "batchnorm35 output shape:\t (1, 6, 24, 24)\n",
      "sigmoid32 output shape:\t (1, 6, 24, 24)\n",
      "pool16 output shape:\t (1, 6, 12, 12)\n",
      "conv20 output shape:\t (1, 16, 8, 8)\n",
      "batchnorm36 output shape:\t (1, 16, 8, 8)\n",
      "sigmoid33 output shape:\t (1, 16, 8, 8)\n",
      "pool17 output shape:\t (1, 16, 4, 4)\n",
      "dense24 output shape:\t (1, 120)\n",
      "batchnorm37 output shape:\t (1, 120)\n",
      "sigmoid34 output shape:\t (1, 120)\n",
      "dense25 output shape:\t (1, 84)\n",
      "batchnorm38 output shape:\t (1, 84)\n",
      "sigmoid35 output shape:\t (1, 84)\n",
      "dense26 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 看一下输出的样子！\n",
    "X = nd.random.normal(shape=(1,1,28,28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X) # 可以看到，BN层后，数据的形状是无变化的！只是规范化了数据的值。\n",
    "    print(layer.name,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 准确率\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n = nd.array([0]),0\n",
    "    for X,y in data_iter:\n",
    "        y_hat = net(X)\n",
    "        acc_sum += (y_hat.argmax(axis=1) == y).sum()\n",
    "        n+=y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 训练\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,test_iter,\n",
    "         batch_size,trainer,num_epochs):\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 别忘了这一行~要计算的是每一轮epoch所用的时间。\n",
    "        train_l_sum,train_acc_sum,n,start = 0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat,y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            \n",
    "            # 每一轮训练（单位是batch）都要统计这个。\n",
    "            train_l_sum = l.asscalar()\n",
    "            train_acc_sum = (y_hat.argmax(axis=1)==y.astype('float32')).sum().asscalar()\n",
    "            n+=y.size\n",
    "            \n",
    "        # 但是test_acc是走完一轮epoch之后才测试的！\n",
    "        # 也就是，每将数据集全部训练完后才测试net的精准度。\n",
    "        test_acc = evaluate_accuracy(test_iter,net)\n",
    "        print('train_l_sum:',train_l_sum / n)\n",
    "        print('train_acc_sum:',train_acc_sum / n)\n",
    "        print('test_acc:',test_acc)\n",
    "        print('time consume:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'conv19_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'conv19_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm35_gamma' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm35_beta' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'conv20_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'conv20_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm36_gamma' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm36_beta' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense24_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense24_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm37_gamma' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm37_beta' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense25_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense25_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm38_gamma' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'batchnorm38_beta' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense26_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\gluon\\parameter.py:893: UserWarning: Parameter 'dense26_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\io\\../operator/elemwise_op_common.h\", line 135\nMXNetError: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node  at 1-th input: expected float32, got int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0a2e23fe6f76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m train(net,train_iter,test_iter,\n\u001b[1;32m----> 5\u001b[1;33m      batch_size,trainer,num_epochs)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-0ff05be82a8d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, trainer, num_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# 但是test_acc是走完一轮epoch之后才测试的！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 也就是，每将数据集全部训练完后才测试net的精准度。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_l_sum:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_l_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_acc_sum:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-15ca75387ee1>\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(data_iter, net)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;34m\"\"\"x.__eq__(y) <=> x==y <=> mx.nd.equal(x, y) \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36mequal\u001b[1;34m(lhs, rhs)\u001b[0m\n\u001b[0;32m   4110\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4111\u001b[0m         \u001b[0m_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_equal_scalar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4112\u001b[1;33m         None)\n\u001b[0m\u001b[0;32m   4113\u001b[0m     \u001b[1;31m# pylint: enable= no-member, protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36m_ufunc_helper\u001b[1;34m(lhs, rhs, fn_array, fn_scalar, lfn_scalar, rfn_scalar)\u001b[0m\n\u001b[0;32m   3569\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlfn_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3570\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3571\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3572\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3573\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'type %s not supported'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\ndarray\\register.py\u001b[0m in \u001b[0;36mbroadcast_equal\u001b[1;34m(lhs, rhs, out, name, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[1;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mcreate_ndarray_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_np_ndarray_cls\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_np_op\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mxnet\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \"\"\"\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\io\\../operator/elemwise_op_common.h\", line 135\nMXNetError: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node  at 1-th input: expected float32, got int32"
     ]
    }
   ],
   "source": [
    "lr,num_epochs,batch_size = 1.0,5,256\n",
    "net.initialize(init = init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "train(net,train_iter,test_iter,\n",
    "     batch_size,trainer,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jn_mxnet",
   "language": "python",
   "name": "mxnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
